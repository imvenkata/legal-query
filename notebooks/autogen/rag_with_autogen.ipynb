{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"gpt-4o\",\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config_assistant = {\n",
    "    \"model\":\"gpt-4o\",\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list\n",
    "}\n",
    "\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config=llm_config_assistant,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize RAG Agents to use own embedding function\n",
    "#### Customizing Text Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recur_spliter = RecursiveCharacterTextSplitter(separators=[\"\\n\", \"\\r\", \"\\t\"])\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=\"YOUR_API_KEY\",\n",
    "                model_name=\"text-embedding-ada-002\"\n",
    "            )\n",
    "\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n",
    "        \"embedding_function\": openai_ef,\n",
    "        \"custom_text_split_function\": recur_spliter.split_text,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Collection autogen-docs already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m assistant\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mragproxyagent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43massistant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mragproxyagent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is autogen?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/10Academy-training/week11/contract-qa-high-precision-rag/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:1015\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Callable):\n\u001b[0;32m-> 1015\u001b[0m     msg2send \u001b[38;5;241m=\u001b[39m \u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_chat_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msender\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chat_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecipient\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1017\u001b[0m     msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/code/10Academy-training/week11/contract-qa-high-precision-rag/.venv/lib/python3.10/site-packages/autogen/agentchat/contrib/retrieve_user_proxy_agent.py:631\u001b[0m, in \u001b[0;36mRetrieveUserProxyAgent.message_generator\u001b[0;34m(sender, recipient, context)\u001b[0m\n\u001b[1;32m    628\u001b[0m n_results \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_results\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m    629\u001b[0m search_string \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_string\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 631\u001b[0m \u001b[43msender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m sender\u001b[38;5;241m.\u001b[39mproblem \u001b[38;5;241m=\u001b[39m problem\n\u001b[1;32m    633\u001b[0m sender\u001b[38;5;241m.\u001b[39mn_results \u001b[38;5;241m=\u001b[39m n_results\n",
      "File \u001b[0;32m~/code/10Academy-training/week11/contract-qa-high-precision-rag/.venv/lib/python3.10/site-packages/autogen/agentchat/contrib/retrieve_user_proxy_agent.py:555\u001b[0m, in \u001b[0;36mRetrieveUserProxyAgent.retrieve_docs\u001b[0;34m(self, problem, n_results, search_string)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_create:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to create collection.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 555\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_or_create \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/code/10Academy-training/week11/contract-qa-high-precision-rag/.venv/lib/python3.10/site-packages/autogen/agentchat/contrib/retrieve_user_proxy_agent.py:338\u001b[0m, in \u001b[0;36mRetrieveUserProxyAgent._init_db\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     IS_TO_CHUNK \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_db\u001b[38;5;241m.\u001b[39mactive_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vector_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_overwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IS_TO_CHUNK:\n",
      "File \u001b[0;32m~/code/10Academy-training/week11/contract-qa-high-precision-rag/.venv/lib/python3.10/site-packages/autogen/agentchat/contrib/vectordb/chromadb.py:107\u001b[0m, in \u001b[0;36mChromaVectorDB.create_collection\u001b[0;34m(self, collection_name, overwrite, get_or_create)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collection\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Collection autogen-docs already exists."
     ]
    }
   ],
   "source": [
    "assistant.reset()\n",
    "ragproxyagent.initiate_chat(assistant, message=ragproxyagent.message_generator, problem=\"What is autogen?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant.reset()\n",
    "userproxyagent = autogen.UserProxyAgent(name=\"userproxyagent\")\n",
    "userproxyagent.initiate_chat(assistant, message=\"What is autogen?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Agents\n",
    "def initialize_agents(config_list, docs_path=None):\n",
    "    ...\n",
    "    return assistant, ragproxyagent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom embedding model and chunking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "You must give as short an answer as possible.\n",
      "\n",
      "User's question is: {input_question}\n",
      "\n",
      "Context is: {input_context}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import PROMPT_QA\n",
    "print(PROMPT_QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE_QA = \"\"\"\n",
    "You are an experienced Legal Assistant who analyzes legal documents. Your expertise includes extracting facts and integrating information from multiple sources to provide well-supported answers. \n",
    "\n",
    "Guidelines:\n",
    "\n",
    "1. Derive your answer strictly from the provided context. Do not introduce any new information.\n",
    "\n",
    "2. Ensure complete contextuality: Address all aspects of the query, linking back to specific details in the context.\n",
    "\n",
    "3. Avoid phrases like \"In the context provided\" or \"According to my knowledge.\"\n",
    "\n",
    "4. Be concise and to the point, don't starting with phrases like, \"The parties are ...\"\n",
    "\n",
    "5. Write in a professional and legally appropriate manner.\n",
    "\n",
    "6. Avoid statements like \"Let me know if you need more information\" or \"I hope this helps.\"\n",
    "\n",
    "Previous Q & A examples include:\n",
    "\n",
    "  *   **Q:** Who owns the Intellectual Property (IP)?\n",
    "      *   **A:** According to Section 4 of the Undertaking (Appendix A), any Work Product, upon creation, shall be fully and exclusively owned by the Company.\n",
    "  *   **Q:** Is there a non-compete obligation for the Advisor?\n",
    "      *   **A:** Yes, during the term of engagement with the Company and for a period of 12 months thereafter.\n",
    "  *   **Q:** Can the Advisor charge for meal time?\n",
    "      *   **A:** No. Section 6.1 specifies that billable hours do not include meals or travel time.\n",
    "\n",
    "\n",
    "Given the guidelines and examples, please answer the question based on the following context.\n",
    "Context: {input_context}\n",
    "\n",
    "Question: {input_question}\n",
    "\n",
    "Answer:\n",
    "\n",
    "REMEMBER YOU SHOULD BE CONCISE AND STRAIGHT TO THE POINT. USE LEGAL TERMINOLOGY WHERE APPROPRIATE.\n",
    "- MAKE SURE TO REFER TO AND CITE SPECIFIC SECTIONS OF THE DOCUMENTS IN YOUR ANSWER, SUCH AS \"ACCORDING TO SECTION 5.1 of ...\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import gradio as gr\n",
    "\n",
    "def initialize_agents(config_list, docs_path=None):\n",
    "    if isinstance(config_list, gr.State):\n",
    "        _config_list = config_list.value\n",
    "    else:\n",
    "        _config_list = config_list\n",
    "    if docs_path is None:\n",
    "        docs_path = \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\"\n",
    "\n",
    "    assistant = RetrieveAssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        system_message=\"You are a helpful assistant.\",\n",
    "    )\n",
    "\n",
    "    ragproxyagent = RetrieveUserProxyAgent(\n",
    "        name=\"ragproxyagent\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        max_consecutive_auto_reply=5,\n",
    "        retrieve_config={\n",
    "            \"task\": \"code\",\n",
    "            \"docs_path\": docs_path,\n",
    "            \"chunk_token_size\": 2000,\n",
    "            \"model\": _config_list[0][\"model\"],\n",
    "            \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "            \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "            \"customized_prompt\": PROMPT_TEMPLATE_QA,\n",
    "            \"get_or_create\": True,\n",
    "            \"collection_name\": \"autogen_rag\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return assistant, ragproxyagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\"\n",
    "assistant, ragproxyagent = initialize_agents(config_list, docs_path=file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.retrieve_utils import TEXT_FORMATS, get_file_from_url, is_url\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def update_prompt(prompt):\n",
    "    ragproxyagent.customized_prompt = prompt\n",
    "    return prompt\n",
    "\n",
    "def upload_file(file):\n",
    "    return update_context_url(file.name)\n",
    "\n",
    "def update_context_url(context_url):\n",
    "    global assistant, ragproxyagent\n",
    "\n",
    "    file_extension = Path(context_url).suffix\n",
    "    print(\"file_extension: \", file_extension)\n",
    "    if file_extension.lower() not in [f\".{i}\" for i in TEXT_FORMATS]:\n",
    "        return f\"File must be in the format of {TEXT_FORMATS}\"\n",
    "\n",
    "    if is_url(context_url):\n",
    "        try:\n",
    "            file_path = get_file_from_url(\n",
    "                context_url,\n",
    "                save_path=os.path.join(\"/tmp\", os.path.basename(context_url)),\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return str(e)\n",
    "    else:\n",
    "        file_path = context_url\n",
    "        context_url = os.path.basename(context_url)\n",
    "\n",
    "    try:\n",
    "        chromadb.PersistentClient(path=\"/tmp/chromadb\").delete_collection(name=\"autogen_rag\")\n",
    "    except:  # noqa\n",
    "        pass\n",
    "    assistant, ragproxyagent = initialize_agents(config_list, docs_path=file_path)\n",
    "    return context_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant, ragproxyagent = initialize_agents(config_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEOUT = 60\n",
    "\n",
    "def initiate_chat(config_list, problem, queue, n_results=3):\n",
    "    global assistant, ragproxyagent\n",
    "    if isinstance(config_list, gr.State):\n",
    "        _config_list = config_list.value\n",
    "    else:\n",
    "        _config_list = config_list\n",
    "    if len(_config_list[0].get(\"api_key\", \"\")) < 2:\n",
    "        queue.put([\"Hi, nice to meet you! Please enter your API keys in below text boxs.\"])\n",
    "        return\n",
    "    else:\n",
    "        llm_config = (\n",
    "            {\n",
    "                \"request_timeout\": TIMEOUT,\n",
    "                # \"seed\": 42,\n",
    "                \"config_list\": _config_list,\n",
    "                \"use_cache\": False,\n",
    "            },\n",
    "        )\n",
    "        assistant.llm_config.update(llm_config[0])\n",
    "    assistant.reset()\n",
    "    try:\n",
    "        ragproxyagent.initiate_chat(assistant, problem=problem, silent=False, n_results=n_results)\n",
    "        messages = ragproxyagent.chat_messages\n",
    "        messages = [messages[k] for k in messages.keys()][0]\n",
    "        messages = [m[\"content\"] for m in messages if m[\"role\"] == \"user\"]\n",
    "        print(\"messages: \", messages)\n",
    "    except Exception as e:\n",
    "        messages = [str(e)]\n",
    "    queue.put(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chatbot_reply(input_text):\n",
    "    \"\"\"Chat with the agent through terminal.\"\"\"\n",
    "    queue = mp.Queue()\n",
    "    process = mp.Process(\n",
    "        target=initiate_chat,\n",
    "        args=(config_list, input_text, queue),\n",
    "    )\n",
    "    process.start()\n",
    "    try:\n",
    "        # process.join(TIMEOUT+2)\n",
    "        messages = queue.get(timeout=TIMEOUT)\n",
    "    except Exception as e:\n",
    "        messages = [str(e) if len(str(e)) > 0 else \"Invalid Request to OpenAI, please check your API keys.\"]\n",
    "    finally:\n",
    "        try:\n",
    "            process.terminate()\n",
    "        except:  # noqa\n",
    "            pass\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description_text():\n",
    "    return \"\"\"\n",
    "    # Microsoft AutoGen: Retrieve Chat Demo\n",
    "\n",
    "    This demo shows how to use the RetrieveUserProxyAgent and RetrieveAssistantAgent to build a chatbot.\n",
    "\n",
    "    #### [AutoGen](https://github.com/microsoft/autogen) [Discord](https://discord.gg/pAbnFJrkgZ) [Blog](https://microsoft.github.io/autogen/blog/2023/10/18/RetrieveChat) [Paper](https://arxiv.org/abs/2308.08155) [SourceCode](https://github.com/thinkall/autogen-demos)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "global assistant, ragproxyagent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'update'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m      6\u001b[0m config_list \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mconfig_list_from_models(\n\u001b[1;32m      7\u001b[0m                 model_list\u001b[38;5;241m=\u001b[39m[os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-35-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m      8\u001b[0m             )\n\u001b[1;32m     10\u001b[0m llm_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     11\u001b[0m     {\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest_timeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: TIMEOUT,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     },\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[43massistant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m(llm_config[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     20\u001b[0m ragproxyagent\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m config_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'update'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "global assistant, ragproxyagent\n",
    "\n",
    "\n",
    "config_list = autogen.config_list_from_models(\n",
    "                model_list=[os.environ.get(\"MODEL\", \"gpt-35-turbo\")],\n",
    "            )\n",
    "\n",
    "llm_config = (\n",
    "    {\n",
    "        \"request_timeout\": TIMEOUT,\n",
    "        # \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "assistant.llm_config.update(llm_config[0])\n",
    "\n",
    "ragproxyagent._model = config_list[0][\"model\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    config_list, assistant, ragproxyagent = (\n",
    "        gr.State(\n",
    "            [\n",
    "                {\n",
    "                    \"api_key\": \"\",\n",
    "                    \"api_base\": \"\",\n",
    "                    \"api_type\": \"azure\",\n",
    "                    \"api_version\": \"2023-07-01-preview\",\n",
    "                    \"model\": \"gpt-35-turbo\",\n",
    "                }\n",
    "            ]\n",
    "        ),\n",
    "        None,\n",
    "        None,\n",
    "    )\n",
    "    assistant, ragproxyagent = initialize_agents(config_list)\n",
    "\n",
    "    gr.Markdown(get_description_text())\n",
    "    chatbot = gr.Chatbot(\n",
    "        [],\n",
    "        elem_id=\"chatbot\",\n",
    "        bubble_full_width=False,\n",
    "        avatar_images=(None, (os.path.join(os.path.dirname(__file__), \"autogen.png\"))),\n",
    "        # height=600,\n",
    "    )\n",
    "\n",
    "    txt_input = gr.Textbox(\n",
    "        scale=4,\n",
    "        show_label=False,\n",
    "        placeholder=\"Enter text and press enter\",\n",
    "        container=False,\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "\n",
    "        def update_config(config_list):\n",
    "            global assistant, ragproxyagent\n",
    "            config_list = autogen.config_list_from_models(\n",
    "                model_list=[os.environ.get(\"MODEL\", \"gpt-35-turbo\")],\n",
    "            )\n",
    "            if not config_list:\n",
    "                config_list = [\n",
    "                    {\n",
    "                        \"api_key\": \"\",\n",
    "                        \"api_base\": \"\",\n",
    "                        \"api_type\": \"azure\",\n",
    "                        \"api_version\": \"2023-07-01-preview\",\n",
    "                        \"model\": \"gpt-35-turbo\",\n",
    "                    }\n",
    "                ]\n",
    "            llm_config = (\n",
    "                {\n",
    "                    \"request_timeout\": TIMEOUT,\n",
    "                    # \"seed\": 42,\n",
    "                    \"config_list\": config_list,\n",
    "                },\n",
    "            )\n",
    "            assistant.llm_config.update(llm_config[0])\n",
    "            ragproxyagent._model = config_list[0][\"model\"]\n",
    "            return config_list\n",
    "\n",
    "        def set_params(model, oai_key, aoai_key, aoai_base):\n",
    "            os.environ[\"MODEL\"] = model\n",
    "            os.environ[\"OPENAI_API_KEY\"] = oai_key\n",
    "            os.environ[\"AZURE_OPENAI_API_KEY\"] = aoai_key\n",
    "            os.environ[\"AZURE_OPENAI_API_BASE\"] = aoai_base\n",
    "            return model, oai_key, aoai_key, aoai_base\n",
    "\n",
    "        txt_model = gr.Dropdown(\n",
    "            label=\"Model\",\n",
    "            choices=[\n",
    "                \"gpt-4\",\n",
    "                \"gpt-35-turbo\",\n",
    "                \"gpt-3.5-turbo\",\n",
    "            ],\n",
    "            allow_custom_value=True,\n",
    "            value=\"gpt-35-turbo\",\n",
    "            container=True,\n",
    "        )\n",
    "        txt_oai_key = gr.Textbox(\n",
    "            label=\"OpenAI API Key\",\n",
    "            placeholder=\"Enter key and press enter\",\n",
    "            max_lines=1,\n",
    "            show_label=True,\n",
    "            value=os.environ.get(\"OPENAI_API_KEY\", \"\"),\n",
    "            container=True,\n",
    "            type=\"password\",\n",
    "        )\n",
    "        txt_aoai_key = gr.Textbox(\n",
    "            label=\"Azure OpenAI API Key\",\n",
    "            placeholder=\"Enter key and press enter\",\n",
    "            max_lines=1,\n",
    "            show_label=True,\n",
    "            value=os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\"),\n",
    "            container=True,\n",
    "            type=\"password\",\n",
    "        )\n",
    "        txt_aoai_base_url = gr.Textbox(\n",
    "            label=\"Azure OpenAI API Base\",\n",
    "            placeholder=\"Enter base url and press enter\",\n",
    "            max_lines=1,\n",
    "            show_label=True,\n",
    "            value=os.environ.get(\"AZURE_OPENAI_API_BASE\", \"\"),\n",
    "            container=True,\n",
    "            type=\"password\",\n",
    "        )\n",
    "\n",
    "    clear = gr.ClearButton([txt_input, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "\n",
    "        def upload_file(file):\n",
    "            return update_context_url(file.name)\n",
    "\n",
    "        upload_button = gr.UploadButton(\n",
    "            \"Click to upload a context file or enter a url in the right textbox\",\n",
    "            file_types=[f\".{i}\" for i in TEXT_FORMATS],\n",
    "            file_count=\"single\",\n",
    "        )\n",
    "\n",
    "        txt_context_url = gr.Textbox(\n",
    "            label=\"Enter the url to your context file and chat on the context\",\n",
    "            info=f\"File must be in the format of [{', '.join(TEXT_FORMATS)}]\",\n",
    "            max_lines=1,\n",
    "            show_label=True,\n",
    "            value=\"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n",
    "            container=True,\n",
    "        )\n",
    "\n",
    "    txt_prompt = gr.Textbox(\n",
    "        label=\"Enter your prompt for Retrieve Agent and press enter to replace the default prompt\",\n",
    "        max_lines=40,\n",
    "        show_label=True,\n",
    "        value=PROMPT_CODE,\n",
    "        container=True,\n",
    "        show_copy_button=True,\n",
    "    )\n",
    "\n",
    "    def respond(message, chat_history, model, oai_key, aoai_key, aoai_base):\n",
    "        global config_list\n",
    "        set_params(model, oai_key, aoai_key, aoai_base)\n",
    "        config_list = update_config(config_list)\n",
    "        messages = chatbot_reply(message)\n",
    "        _msg = (\n",
    "            messages[-1]\n",
    "            if len(messages) > 0 and messages[-1] != \"TERMINATE\"\n",
    "            else messages[-2]\n",
    "            if len(messages) > 1\n",
    "            else \"Context is not enough for answering the question. Please press `enter` in the context url textbox to make sure the context is activated for the chat.\"\n",
    "        )\n",
    "        chat_history.append((message, _msg))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    def update_prompt(prompt):\n",
    "        ragproxyagent.customized_prompt = prompt\n",
    "        return prompt\n",
    "\n",
    "    def update_context_url(context_url):\n",
    "        global assistant, ragproxyagent\n",
    "\n",
    "        file_extension = Path(context_url).suffix\n",
    "        print(\"file_extension: \", file_extension)\n",
    "        if file_extension.lower() not in [f\".{i}\" for i in TEXT_FORMATS]:\n",
    "            return f\"File must be in the format of {TEXT_FORMATS}\"\n",
    "\n",
    "        if is_url(context_url):\n",
    "            try:\n",
    "                file_path = get_file_from_url(\n",
    "                    context_url,\n",
    "                    save_path=os.path.join(\"/tmp\", os.path.basename(context_url)),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                return str(e)\n",
    "        else:\n",
    "            file_path = context_url\n",
    "            context_url = os.path.basename(context_url)\n",
    "\n",
    "        try:\n",
    "            chromadb.PersistentClient(path=\"/tmp/chromadb\").delete_collection(name=\"autogen_rag\")\n",
    "        except:  # noqa\n",
    "            pass\n",
    "        assistant, ragproxyagent = initialize_agents(config_list, docs_path=file_path)\n",
    "        return context_url\n",
    "\n",
    "    txt_input.submit(\n",
    "        respond,\n",
    "        [txt_input, chatbot, txt_model, txt_oai_key, txt_aoai_key, txt_aoai_base_url],\n",
    "        [txt_input, chatbot],\n",
    "    )\n",
    "    txt_prompt.submit(update_prompt, [txt_prompt], [txt_prompt])\n",
    "    txt_context_url.submit(update_context_url, [txt_context_url], [txt_context_url])\n",
    "    upload_button.upload(upload_file, upload_button, [txt_context_url])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True, server_name=\"0.0.0.0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
